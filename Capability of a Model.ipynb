{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification, kNN, Cross-validation, Dimensionality Reduction\n",
    "\n",
    "# [presentation slides](https://github.com/cs109/2015/blob/master/Lectures/09-ClassificationPCA.pdf)\n",
    "\n",
    "## [CS 109 Classification & PCA](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=c322c0d5-9cf9-4deb-b59f-d6741064ba8a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "In the simplest form:\n",
    "\n",
    "- We start with data\n",
    "- We add labels\n",
    "\n",
    "[MIT Scene Recognition Demo](http://places.csail.mit.edu/demo.html)\n",
    "\n",
    "- Uses data as input\n",
    "- references a large labeled database to make inferences\n",
    "- Machine learning have become exceedingly good at image classification\n",
    "- kNN (k-Nearest Neighbours Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Fundamentally:\n",
    "\n",
    "- Input: A **training set** of N data points, each labeled with one of K different classes.\n",
    "- Learning: Use the training set to learn what every one of the classes looks like.\n",
    "- Evaluation: Predict labels for a **test set** of data and compare the true labels (ground truth) to the ones predicted by the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "- Examples: kNN, SVM, Decision Trees, Random Forests, Bagging, Boosting, etc.\n",
    "- Make predictions for new data points\n",
    "- Data has labels (categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "- Examples: PCA, MDS, Clustering, etc.\n",
    "- Find patterns in the data\n",
    "- Data has no labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning we are looking for the **decision boundry** between our classifications. Data points are generally labelled as `x` and labels are `y`\n",
    "\n",
    "These datasets can exist in high dimensional space and the space is spand by **features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "- If trying to classify Apples and Oranges based on roundness and weight you would have a near impossible task\n",
    "\n",
    "- If you choose colour and shape as your features then the classes begin to segregate more easily.\n",
    "\n",
    "    More of an art than a science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbour Classifier\n",
    "\n",
    "- Essentially a lookup\n",
    "\n",
    "- Predict class of new data point by majority vote of K nearest neighbours\n",
    "\n",
    "- K is primarily an odd number so that their is a clear winner\n",
    "\n",
    "- [Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram)\n",
    "\n",
    "![diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Euclidean_Voronoi_diagram.svg/220px-Euclidean_Voronoi_diagram.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I-NN Properties (One Nearest Neighbour):\n",
    "\n",
    "- Simple and quite good for low dimensional data\n",
    "- \"Rough\" decision boundary, may have \"islands\"\n",
    "- Training complexity for N data points? order 1\n",
    "- Test complexity for M data points? N â€¢ M\n",
    "- Error on training set? zero (always finding the NN)\n",
    "- Variance? Bias?\n",
    "    - The variance is quite high because for each new training set I get a new decision boundary\n",
    "    - The bias is quite low because, on average, the tests do quite well\n",
    "    - To reduce the variance we are going to increase the number of neighbours\n",
    "    \n",
    "- As we increase k, variance will decrease but the bias will potentially increase.\n",
    "\n",
    "- **How do we choose the ideal k?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "- You split your data into a training set and a test set. Then you use your training set to evaluate your test set for performance.\n",
    "\n",
    "- k is a hyper parameter\n",
    "\n",
    "- How many datapoints need to move to the training and test?\n",
    "\n",
    "- How do we define the distance function for nearest neighbours?\n",
    "\n",
    "- Pick the k with the lowest test error\n",
    "\n",
    "- We don't want to use all our training data to test the testing data because, statistically speaking, this is a single sample size. Therefore we divide our traning data into folds:\n",
    "\n",
    "![cross validation folds](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "- Training data: train classifier\n",
    "- Validation data: estimate (hyper) parameters (k) (one of the training folds)\n",
    "- Test data: measure performance\n",
    "    \n",
    "#### Process:\n",
    "    1. Iterate over choice of validation fold\n",
    "    2. For all parameter values:\n",
    "        1. Train with training data\n",
    "        2. Validate with validation data\n",
    "    3. Average the parameters with best performance of validation data\n",
    "    \n",
    "- 5 fold and 10 fold are typical numbers for training/validation but it comes down to intuition and experience for a particular classifier.\n",
    "\n",
    "## The test data is NOT used to determine the parameters\n",
    "\n",
    "- do not touch your test data until the very end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Process:\n",
    "\n",
    "1. Take best parameters\n",
    "2. Train on training data and validation data together\n",
    "3. Test performance on test data\n",
    "\n",
    "#### Evaluate on the test set only a single time, at the very end.\n",
    "\n",
    "- The results will not be generalisable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagevector](vector.png)\n",
    "\n",
    "3,072 dimensions\n",
    "\n",
    "![distance](distance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "- To improve the accuracy we need to consider new **features** (other than pixel vectors)\n",
    "\n",
    "- Our most sophisticated feature analysis for images is based on neural network algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why don't we just use more and more features?\n",
    "\n",
    "- Curse of Dimensionality\n",
    "\n",
    "- When dimensionality increases, the volume of the space increases so fast that the available data becomes sparse\n",
    "\n",
    "- Statistically sound result requires the sample size N to grow exponentially with d\n",
    "\n",
    "## What can we do?\n",
    "\n",
    "- We can try to work the problem down into a lower dimensional form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "- We are trying to maintain the distance\n",
    "\n",
    "#### Basically\n",
    "\n",
    "- Project the high-dimensional data onto a lower-dimensional subspace that best \"fits\" the data.\n",
    "\n",
    "- Many methods are linear projections\n",
    "\n",
    "### Does the data lie mostly in a hyperplane? If so, what is its *intrinsic* dimensionality d?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (Principal Component Analysis)\n",
    "\n",
    "- We want to choose our principal components such that we minimize orthogonal distances\n",
    "\n",
    "v: chosen to minimize orthogonal distances\n",
    "\n",
    "Equivalent: v is the direction of maximum variance (max. the spread along v)\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "1. Substract mean from data (center X)\n",
    "2. (Typically) scale each dimension by its variance\n",
    "    1. Helps to pay less attention to magnitude of dimensions\n",
    "3. Compute covariance matrix S\n",
    "4. Compute k largest eigenvectors of S\n",
    "5. These eigenvectors are the k principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA will tell you the intrinsic dimensionality of your dataset\n",
    "\n",
    "There is a simple formula for the variance that each eigenvector explains\n",
    "\n",
    "![pcvectors](pcvectors.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDS Multi-dimentional Scaling\n",
    "\n",
    "## A different goal\n",
    "\n",
    "- Find a set of points whose pairwise distances match a given distance matrix.\n",
    "\n",
    "- Given n x n matrix of pairwise distances between data points\n",
    "- Compute n x k matrix X with coordinates of distances with some linear algebra magic\n",
    "- Perform PCA on this matrix X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
