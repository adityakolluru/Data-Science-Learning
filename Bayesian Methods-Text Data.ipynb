{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics\n",
    "\n",
    "## [Slides](https://github.com/cs109/2015/blob/master/Lectures/16-BayesianMethods.pdf)\n",
    "\n",
    "### Recommended Reading\n",
    "[The theory that would not die](https://itunes.apple.com/us/book/the-theory-that-would-not-die/id646319489?mt=11)\n",
    "\n",
    "[Think Bayes](https://itunes.apple.com/us/book/think-bayes/id705489536?mt=11)\n",
    "\n",
    "[Probabilisitic Programming and Bayesian Methods for Hackers](https://itunes.apple.com/us/book/bayesian-methods-for-hackers/id1045072989?mt=11)\n",
    "\n",
    "- [iPython Notebook](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\n",
    "\n",
    "[Doing Bayesian Data Analysis](https://www.amazon.ca/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884/ref=dp_ob_title_bk)\n",
    "\n",
    "[Bayesian Data Analysis](https://www.amazon.ca/Bayesian-Data-Analysis-Andrew-Gelman/dp/1439840954/ref=pd_bxgy_14_img_2?_encoding=UTF8&pd_rd_i=1439840954&pd_rd_r=b4c8b1a6-cd71-11e8-ab44-9143b7b890b1&pd_rd_w=FreZI&pd_rd_wg=RXthh&pf_rd_i=desktop-dp-sims&pf_rd_m=A3DWYIK6Y9EEQB&pf_rd_p=cda2b2aa-f379-4b98-b5ff-b78659186dbe&pf_rd_r=X4FZ259D7N4VZ1CY1P4P&pf_rd_s=desktop-dp-sims&pf_rd_t=40701&psc=1&refRID=X4FZ259D7N4VZ1CY1P4P)\n",
    "\n",
    "# Bayes' rule\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "$P(A)$ is your prior belief\n",
    "$P(B)$ is new information (something you observed)\n",
    "- Now you want to update your belief\n",
    "\n",
    "$P(A|B)$ is the posterior probability for $A$\n",
    "\n",
    "## Statistical Notation\n",
    "$$p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)}$$\n",
    "\n",
    "Treating the data y as fixed,\n",
    "\n",
    "$$p(\\theta|y) \\propto L(\\theta)p(\\theta)$$\n",
    "\n",
    "Bayes' rule says the posterior density is proportional to the likelihood function times the prior density.\n",
    "\n",
    "**Likelihood** in statistics means that you are taking the probability of the data given the parameter but you are viewing that probability as a function of the parameter. \n",
    "\n",
    "The classical frequentist is used to thinking of $\\theta$ as an unknown constant and they are very uncomfortable giving it a probability distribution. \n",
    "\n",
    "The Bayesian perspective is that $\\theta$ is unknown we want to quantify it's uncertainty and propability is the best way to do that.\n",
    "\n",
    "A Bayesian is, in essense, treating everything as a random variable.\n",
    "\n",
    "### The ongoing debate is:\n",
    "Where does the prior come from -- $p(\\theta)$\n",
    "\n",
    "Bayesians are thinking about the rational agents that can subjectively impart a prior.\n",
    "\n",
    "# Discriminative vs. Generative Classifiers\n",
    "\n",
    "- What to model and what not to model?\n",
    "\n",
    "**Discriminative:** just model $p(y|x)$\n",
    "\n",
    "**Generative:** give a full probability model $p(x,y)=p(x)p(y|x)=p(y)p(x|y)$\n",
    "\n",
    "# Naive Bayes Spam Filter\n",
    "- Naive Bayes assumption: conditional independence given spam, and also conditional independence given not spam.\n",
    "\n",
    "# Full Probability Modeling\n",
    "The process of Bayesian data analysis can be idealized by dividing it into the following three steps:\n",
    "1. Setting up a full probability model - a joint probability distribution for all (observed,) observable and unobservable quantities in a problem...\n",
    "2. Conditioning on observed data: calculating and interpreting the appropriate posterior distribution - the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data\n",
    "3. Evaluating the fit of the model and the implications of the resulting posterior distribution...\n",
    "\n",
    "### Think like a Bayesian, check like a frequentist.\n",
    "\n",
    "# Conjugate Priors: Beta-Binomial\n",
    "\n",
    "$X$ - observed data\n",
    "\n",
    "$p$ - parameter\n",
    "\n",
    "$\\text{Bin}$ - Binomial\n",
    "\n",
    "$n$ - sample size\n",
    "\n",
    "$$X|p \\sim Bin(n,p)$$ \n",
    "\n",
    "$\\beta$ - The density\n",
    "\n",
    "$$p \\sim Beta(a,b)$$\n",
    "\n",
    "- $Beta(a,b)$ [density](https://en.wikipedia.org/wiki/Beta-binomial_distribution):\n",
    "\n",
    "$$f(p) \\propto p^{a-1}(1-p)^{b-1}$$\n",
    "![distribution](https://upload.wikimedia.org/wikipedia/commons/e/e1/Beta-binomial_distribution_pmf.png)\n",
    "\n",
    "Conjugate Priors are extremely convenient. Beta is a whole family of distributions. If you start out in that family and then observe data, you are still in that family.\n",
    "\n",
    "Posterior is then $p|X = x \\sim Beta(a + x, b + n - x)$\n",
    "\n",
    "# Conjugate Priors: Normal-Normal\n",
    "\n",
    "### Normal is [conjugate](https://en.wikipedia.org/wiki/Conjugate_prior) to itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "- Tune number of trees and number of features\n",
    "- Rule of Thumb: The number of features on the $\\sqrt{\\text{Number of Features}}$\n",
    "\n",
    "## Out of Bag Error\n",
    "- Very similar to cross-validation\n",
    "- Measured during training\n",
    "- Can be too optimistic\n",
    "\n",
    "# Variable Importance\n",
    "- Again use out of bag samples\n",
    "- Predict class for these samples\n",
    "- Randomly permute values of one feature\n",
    "- Predict classes again\n",
    "- Measure decrease in accuracy\n",
    "\n",
    "## Continued...\n",
    "- Measure split criterion improvement\n",
    "- Record importements for each feature\n",
    "- Accumulate over whole ensemble\n",
    "\n",
    "# Regression\n",
    "- Preserves the distance between the labels\n",
    "- A classification problem does not concern itself with \"is 1 star closer to 3 stars or 5 stars\" in a ranking system.\n",
    "# k-NN\n",
    "- How would you modify k-NN for Regression?\n",
    "    - Average the values of the K-NN\n",
    "    - Or, build a weighted average of the k-NN\n",
    "\n",
    "```python\n",
    "KNeighborsRegressor(k = 5, weights = 'uniform')\n",
    "```\n",
    "- Ends up being quite discrete in the predictions\n",
    "\n",
    "```python\n",
    "KNeighborsRegressor(k = 5, weights = 'distance')\n",
    "```\n",
    "- You weight by $\\frac{1}{\\text{distance}}$\n",
    "    - This is the default in scikit-learn\n",
    "- You end up overweighting outliers and including them all\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "# Regression Tree\n",
    "- You cannot use the gini impurity\n",
    "    - Now you use your squared error\n",
    "- Again we average, this time over all points in one of the cells\n",
    "- During training, split in the way that reduces the squared error the most\n",
    "- Same idea as before\n",
    "- Train multiple trees in parallel and average\n",
    "- Different defaults\n",
    "- max_features = n_features (_need to check this in the scikit-learn documentation_)\n",
    "- square error\n",
    "\n",
    "# SVM for Regression\n",
    "- The original incarnation of SVM was purely classification\n",
    "- We need to rethink the optimization problem\n",
    "\n",
    "## Boosting\n",
    "- Addes weighting to data and smoothes out the regression prediction.\n",
    "\n",
    "# Best Practices\n",
    "- Typical Progress in a machine learning problem is to make high gains early and then fight hard for small gains until the end of the project.\n",
    "\n",
    "## Generally\n",
    "- It will be harder than it looks\n",
    "- Know your applicaiton:\n",
    "    - zero values\n",
    "    - outliers\n",
    "    - where do labels come from\n",
    "        - most humans generate labels and there is judgement bias. The source of a lot of noise.\n",
    "- Document, document, document\n",
    "    - for yourself and for others\n",
    "- commit, pull, push, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
